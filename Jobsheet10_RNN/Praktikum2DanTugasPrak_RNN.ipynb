{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCOgnP96L7sU"
   },
   "source": [
    "# Praktikum 2 - Generator Teks dengan RNN\n",
    "\n",
    "**Anggota Kelompok :**\n",
    "\n",
    "**1. Daffa Yudisa Akbar /\t\t\tNIM. 2241720008**\n",
    "\n",
    "**2. Hafizh Muhammad Rabbani /\tNIM. 2241720242**\n",
    "\n",
    "**3. Joyo Sugito /\t\t\t\tNIM. 2241720050**\n",
    "\n",
    "**4. Rendy Putra Kusuma /\t\tNIM. 2241720124**\n",
    "\n",
    "Praktikum ini mendemonstrasikan cara melakukan genearsi text menggunakan RNN. Dataset yang digunkan adalah dataset Shakespeare's writing from Andrej Karpathy's The Unreasonable Effectiveness of Recurrent Neural Networks. Jika diberikan urutan karakter dari data ini (\"Shakespear\"), latih model untuk memprediksi karakter berikutnya dalam urutan (\"e\"). Urutan teks yang lebih panjang dapat dihasilkan dengan memanggil model berulang kali.\n",
    "\n",
    "Note: Enable GPU acceleration to execute this notebook faster. In Colab: Runtime > Change runtime type > Hardware accelerator > GPU.\n",
    "\n",
    "Tutorial ini menggunakan tf.keras dan eager execution. Berikut adalah contoh output ketika model dalam tutorial ini dilatih selama 30 epoch, dan dimulai dengan prompt \"Q\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bJhNoZ9NjHK"
   },
   "source": [
    "Meskipun beberapa kalimat memiliki tata bahasa, sebagian besar tidak masuk akal. Model belum mempelajari arti kata-kata, namun anggap saja:\n",
    "\n",
    "- Modelnya berbasis karakter. Saat pelatihan dimulai, model tidak mengetahui cara mengeja kata dalam bahasa Inggris, atau bahkan kata-kata tersebut merupakan satuan teks.\n",
    "\n",
    "- Struktur keluarannya menyerupai sandiwara—blok teks umumnya dimulai dengan nama pembicara, dengan huruf kapital semua mirip dengan kumpulan data.\n",
    "\n",
    "- Seperti yang ditunjukkan di bawah, model dilatih pada kumpulan teks kecil (masing-masing 100 karakter), dan masih mampu menghasilkan rangkaian teks yang lebih panjang dengan struktur yang koheren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_jA3Q2UNnzJ"
   },
   "source": [
    "## Setup\n",
    "\n",
    "### Import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IKyA6HAGRkx0",
    "outputId": "47e0298e-2212-47e6-dec1-ba1f68e3a03b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.15.0\n",
      "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
      "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n",
      "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.12.2)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.64.1)\n",
      "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
      "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
      "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.44.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
      "Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, ml-dtypes, keras, tensorboard, tensorflow\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.16.0\n",
      "    Uninstalling wrapt-1.16.0:\n",
      "      Successfully uninstalled wrapt-1.16.0\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.4.1\n",
      "    Uninstalling ml-dtypes-0.4.1:\n",
      "      Successfully uninstalled ml-dtypes-0.4.1\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.4.1\n",
      "    Uninstalling keras-3.4.1:\n",
      "      Successfully uninstalled keras-3.4.1\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.17.0\n",
      "    Uninstalling tensorboard-2.17.0:\n",
      "      Successfully uninstalled tensorboard-2.17.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.17.0\n",
      "    Uninstalling tensorflow-2.17.0:\n",
      "      Successfully uninstalled tensorflow-2.17.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorstore 0.1.67 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
      "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 wrapt-1.14.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FkhaDgkgNnOX"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N7YRQp9tQmhr",
    "outputId": "443f1393-2794-4c8d-8a01-1f7eeacdeeae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ay5cWBCzNwea"
   },
   "source": [
    "### Download Dataset Shakespeare\n",
    "\n",
    "Sesuaikan dengan lokasi data yang Anda punya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eoxiYXzXNxqO",
    "outputId": "f4107dc3-890b-4210-fdf3-b938f1fe9770"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1115394/1115394 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yat3zbSmOr9L"
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ipx_JC8EOuID",
    "outputId": "68fa26d3-e2a9-4360-c3d8-c095bab07ad5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BUT2ckfLOvjZ",
    "outputId": "db02aaec-6e49-48ff-d6d5-5b93581e75fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RZ3o_EZEOyFT",
    "outputId": "03007015-af67-4e45-deca-e447834b0d9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9yKt8bUO1mY"
   },
   "source": [
    "## Olah Teks\n",
    "\n",
    "### Vectorize Teks\n",
    "\n",
    "Sebelum training, Anda perlu mengonversi string menjadi representasi numerik. tf.keras.layers.StringLookup dapat mengubah setiap karakter menjadi ID numerik. Caranya adalah teks akan dipecah menjadi token terlebih dahulu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jFs15kC3O6r6",
    "outputId": "313f6427-8267-488a-b51f-1ab8aa93570d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSU5KnyTPGyS"
   },
   "source": [
    "sekarang buat tf.keras.layers.StringLookup layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MGEelC0tPHz8"
   },
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "vocabulary=list(vocab), mask_token=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-usYaoFPNWL"
   },
   "source": [
    "perintah diatas mengconvert token menjadi id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UzVhD_Y5POcf",
    "outputId": "3804eb98-fea6-44ee-f601-6c4465c2cd48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8omcxZcPTyy"
   },
   "source": [
    "Karena tujuan tutorial ini adalah untuk menghasilkan teks, penting juga untuk membalikkan representasi ini. Untuk ini Anda dapat menggunakan kode tf.keras.layers.StringLookup(..., invert=True).\n",
    "\n",
    "Catatan: pada kode ini, daripada meneruskan kosakata asli yang dihasilkan dengan diurutkan(set(teks)) gunakan metode get_vocabulary() dari tf.keras.layers.StringLookup sehingga token [UNK] disetel dengan cara yang sama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "AG6f_WVxPVGn"
   },
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81OA974pPW6F"
   },
   "source": [
    "Lapisan ini mengconvert kembali karakter dari vektor ID, dan mengembalikannya sebagai karakter tf.RaggedTensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jE2iLt31PYAH",
    "outputId": "15b2542e-ccc0-44dc-bded-3282dfe038fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEtezbOdPbk_"
   },
   "source": [
    "Anda dapat menggunakan tf.strings.reduce_join untuk menggabungkan kembali karakter menjadi string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FhuXIoluPeQS",
    "outputId": "a9a0bcd3-0df9-4d23-b1ff-9f72aebb7624"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "S0NFmU9BPgvl"
   },
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kz0Js_lYPqT-"
   },
   "source": [
    "### Prediksi\n",
    "\n",
    "Diberikan sebuah karakter, atau serangkaian karakter, karakter apa yang paling mungkin berikutnya? Ini adalah tugas yang harus Anda latih agar model dapat melakukannya. Masukan ke model akan berupa urutan karakter, dan Anda melatih model untuk memprediksi keluaran berupa karakter berikut pada setiap langkah waktu. Karena RNN mempertahankan keadaan internal yang bergantung pada elemen yang terlihat sebelumnya, mengingat semua karakter dihitung hingga saat ini, karakter apa selanjutnya?\n",
    "\n",
    "### Membuat Trianing Set dan Target\n",
    "\n",
    "Selanjutnya bagilah teks menjadi contoh sequence. Setiap masukan sequence akan berisi karakter seq_length dari teks. Untuk setiap masukan sequence, target prediksi berisi teks dengan panjang yang sama, hanya digeser satu karakter ke kanan. Jadi, bagi teks menjadi beberapa bagian seq_length+1. Misalnya, seq_length adalah 4 dan teks kita adalah \"Hello\". Urutan masukannya adalah \"Hell\", dan urutan targetnya adalah \"ello\". Untuk melakukan ini, pertama-tama gunakan fungsi tf.data.Dataset.from_tensor_slices untuk mengonversi vektor teks menjadi aliran indeks karakter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aPflJ1AVPzjC",
    "outputId": "25dcb62a-ab3b-43d2-b067-d5ea9a2a8cf5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "dxP1cEhYP5Au"
   },
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XwxjKWxiP8bT",
    "outputId": "37d50d7b-bed9-4d3c-f2b5-0f3f72e7220c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "zWU_fKgtP_7X"
   },
   "outputs": [],
   "source": [
    "seq_length = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3t0q8zbQjXg"
   },
   "source": [
    "Metode batch memungkinkan Anda dengan mudah mengonversi karakter individual ini menjadi urutan ukuran yang diinginkan.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Lj5bICrQkfi",
    "outputId": "6ea2a7fc-2656-427b-bd48-9066e421517c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqPiPwNvQm2V"
   },
   "source": [
    "akan lebih mudah untuk melihat apa yang dilakukan jika Anda menggabungkan token kembali menjadi string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2o15f5ZMQpQE",
    "outputId": "341814da-a1c7-4ccb-ec78-25b41328b9f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "    print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adhPdm5rQw-O"
   },
   "source": [
    "Untuk pelatihan, Anda memerlukan kumpulan data pasangan (input, label). Dimana input dan label merupakan urutan. Pada setiap langkah waktu, inputnya adalah karakter saat ini dan labelnya adalah karakter berikutnya. Berikut adalah fungsi yang mengambil urutan sebagai masukan, menduplikasi, dan menggesernya untuk menyelaraskan masukan dan label untuk setiap langkah waktu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "9pHVTDumQyIU"
   },
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "  input_text = sequence[:-1]\n",
    "  target_text = sequence[1:]\n",
    "  return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vPRy1IsOQ6oz",
    "outputId": "8ef1f0c8-2c27-40d2-fed4-cc6097c212fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YjNaGOdaQ7uY",
    "outputId": "9e6976bd-bdf4-4d3f-8002-bb87ad2f0415"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "VACKE4sLRAZL"
   },
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "svbNHGDCQ9ht",
    "outputId": "97e3c6c1-c535-4d82-a30a-e7bec8f598be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDhADSIoRHba"
   },
   "source": [
    "### Membuat Batch Training\n",
    "\n",
    "Anda menggunakan tf.data untuk membagi teks menjadi sequence yang dapat diatur. Namun sebelum memasukkan data ini ke dalam model, Anda perlu mengacak data dan mengemasnya ke dalam batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fZ5CGH24RhY-",
    "outputId": "4b3bbf57-d802-4d37-a8cf-d225b43b14e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBVJNnfHRkqY"
   },
   "source": [
    "## Buat Model\n",
    "\n",
    "Bagian ini mendefinisikan model sebagai subkelas keras.Model (untuk lebih detilnya, lihat Making new Layers and Models via subclassing).\n",
    "\n",
    "Model yang kita bangun memiliki 3 lapisan neural network :\n",
    "\n",
    "  * tf.keras.layers.Embedding: Lapisan masukan. Tabel pencarian yang dapat dilatih yang akan memetakan setiap karakter-ID ke vektor dengan dimensi embedding_dim;\n",
    "\n",
    "  * tf.keras.layers.GRU: lapisan RNN dengan ukuran unit=rnn_units (Anda juga dapat menggunakan lapisan LSTM di sini.)\n",
    "\n",
    "  * tf.keras.layers.Dense: Lapisan keluaran, dengan keluaran vocab_size. Ini menghasilkan satu logit untuk setiap karakter dalam kosakata. Ini adalah log kemungkinan setiap karakter menurut model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "oDhk6OGtRldU"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "AtJxS2x6RtyA"
   },
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                      return_sequences=True,\n",
    "                                      return_state=True,\n",
    "                                      stateful=False)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "\n",
    "        # Ensure states has correct shape if provided\n",
    "        if states is not None:\n",
    "            # Reshape states to match expected dimensions (batch_size, units)\n",
    "            states = tf.convert_to_tensor(states)\n",
    "            if len(states.shape) == 1:\n",
    "                states = tf.expand_dims(states, axis=0)\n",
    "        else:\n",
    "            # Initialize states with zeros if not provided\n",
    "            batch_size = tf.shape(x)[0]\n",
    "            states = tf.zeros((batch_size, self.gru.units))\n",
    "\n",
    "        # Run GRU layer\n",
    "        outputs = self.gru(x,\n",
    "                         initial_state=states,\n",
    "                         training=training)\n",
    "\n",
    "        # Unpack GRU outputs\n",
    "        sequence_output = outputs[0]  # shape: (batch_size, sequence_length, units)\n",
    "        final_state = outputs[1]      # shape: (batch_size, units)\n",
    "\n",
    "        # Dense layer for predictions\n",
    "        x = self.dense(sequence_output, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, final_state\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "WsZ-UBnsR475"
   },
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sl0KidFhTMxu"
   },
   "source": [
    "Untuk setiap karakter, model mencari penyematan, menjalankan GRU satu langkah waktu dengan penyematan sebagai masukan, dan menerapkan dense layer untuk menghasilkan log yang memprediksi kemungkinan log karakter berikutnya:\n",
    "\n",
    "Note: Untuk pelatihan Anda bisa menggunakan model keras.Sequential di sini. Untuk menghasilkan teks nanti, Anda harus mengelola status internal RNN. Akan lebih mudah untuk memasukkan opsi input dan output status di awal, daripada mengatur ulang arsitektur model nanti. untuk detailnya bisa dilihat Keras RNN guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8i0riO24TPj1"
   },
   "source": [
    "## Uji Model\n",
    "\n",
    "Coba jalankan model dan cek apakah sidah sesuai dengan output\n",
    "\n",
    "pertama, cek bentuk dari output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ze2nKbqGTeFZ",
    "outputId": "031031d6-129f-44a0-c698-710824d5a931"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_I0dVbGXtpN"
   },
   "source": [
    "Dalam contoh di atas, panjang urutan masukan adalah 100 tetapi model dapat dijalankan pada masukan dengan panjang berapa pun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MEgFklP3XvSb",
    "outputId": "882166d7-3edf-4610-bf8e-81f2cf498a91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16896     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4022850 (15.35 MB)\n",
      "Trainable params: 4022850 (15.35 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wAfZqyuXxYT"
   },
   "source": [
    "Untuk mendapatkan prediksi aktual dari model, Anda perlu mengambil sampel dari distribusi keluaran, untuk mendapatkan indeks karakter aktual. Distribusi ini ditentukan oleh logit pada kosakata karakter. Catatan: Penting untuk mengambil sampel dari distribusi ini karena mengambil argmax dari distribusi tersebut dapat dengan mudah membuat model terjebak dalam infinote loop. Cobalah untuk contoh pertama di batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "ur1RFgU4Xyvi"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ikh50jlVX3eS"
   },
   "source": [
    "Hal ini memberi kita, pada setiap langkah waktu, prediksi indeks karakter berikutnya:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8gbTKSN6X4oC",
    "outputId": "ac241b3f-ecba-44d0-9cfd-67de70acd467"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28, 23, 16, 40, 37, 39, 16, 56,  1, 44, 10, 25,  6,  9, 44, 29, 52,\n",
       "        7,  6, 34, 32,  0, 57,  1, 36, 20, 33, 35, 28,  0, 35, 51, 53, 31,\n",
       "       45, 59, 46, 47, 29, 17,  7,  6, 16, 63, 57, 44, 59, 38, 24,  7, 25,\n",
       "       28, 18, 19, 60, 62, 62, 59, 56, 21, 32,  1, 40, 56, 55, 65,  6, 10,\n",
       "       19, 12, 46,  9,  5, 18,  9, 56, 52, 12, 58, 26, 19,  0, 13, 11,  4,\n",
       "       60, 48, 23, 11, 26, 10, 58, 45, 60, 31, 58, 32, 61, 38, 54])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpxIS_QlX5tj"
   },
   "source": [
    "Dekode kode berikut untuk melihat teks yang diprediksi oleh model tidak terlatih ini:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dv2v5Z7EX6_7",
    "outputId": "6240372f-c0be-4e99-c738-ad76efa10fcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b\"S:\\nMy lords, at once: the cause why we are met\\nIs, to determine of the coronation.\\nIn God's name, sp\"\n",
      "\n",
      "Next Char Predictions:\n",
      " b\"OJCaXZCq\\ne3L'.ePm,'US[UNK]r\\nWGTVO[UNK]VlnRftghPD,'CxretYK,LOEFuwwtqHS\\naqpz'3F;g.&E.qm;sMF[UNK]?:$uiJ:M3sfuRsSvYo\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1w6VKUPX93F"
   },
   "source": [
    "## Train Model\n",
    "\n",
    "Pada titik ini permasalahan dapat dianggap sebagai permasalahan klasifikasi standar. Permasalahan dapat disimpulkan dengan : Berdasarkan status RNN sebelumnya, dan masukan langkah kali ini, prediksi kelas karakter berikutnya.\n",
    "\n",
    "### Tambahan optimizer dan fungsi loss\n",
    "\n",
    "loss function tf.keras.losses.sparse_categorical_crossentropy standar berfungsi dalam kasus ini karena diterapkan di seluruh dimensi terakhir prediksi. Karena model Anda mengembalikan logits, Anda perlu mengatur flag from_logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "01yHce6GYKoO"
   },
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VyLzfnsUYM2m",
    "outputId": "69233881-1cbd-4066-ad7a-5b69068eac5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.1896935, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01HHMkuLYOth"
   },
   "source": [
    "Model yang baru diinisialisasi tidak boleh terlalu yakin dengan dirinya sendiri, semua log keluaran harus memiliki besaran yang sama. Untuk mengonfirmasi hal ini, Anda dapat memeriksa bahwa eksponensial dari loss rata-rata harus kira-kira sama dengan ukuran kosakata. Loss yang jauh lebih tinggi berarti model tersebut yakin akan jawaban yang salah, dan memiliki inisialisasi yang buruk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xy79dumvYQkp",
    "outputId": "9e9c1c5a-3fb8-43d2-db31-5d0244e029c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.002556"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwsVRbjNYSAS"
   },
   "source": [
    "Konfigurasikan prosedur pelatihan menggunakan metode tf.keras.Model.compile. Gunakan tf.keras.optimizers.Adam dengan argumen default dan fungsi loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "O-XunXjGYTe0"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "raNe81MZYUjd"
   },
   "source": [
    "### Konfigurasi Checkpoints\n",
    "\n",
    "Gunakan tf.keras.callbacks.ModelCheckpoint untuk memastikan bahwa checkpoint disimpan selama pelatihan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "q4_0gRdOYXMM"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "# Name of the checkpoint files with correct format string placement\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir,\"ckpt_{epoch}.weights.h5\")\n",
    "\n",
    "# Create the checkpoint callback\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUp8gQEqZRYh"
   },
   "source": [
    "## Lakukan Proses Training\n",
    "\n",
    "Agar waktu pelatihan tidak terlalu lama, gunakan 10 epoch untuk melatih model. Di Colab, setel runtime ke GPU untuk pelatihan yang lebih cepat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "dT-7awqLZUwM"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cnE-b7XnZWbH",
    "outputId": "1de07e6e-5d22-45f4-9c64-2d141ca9ff6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "172/172 [==============================] - ETA: 0s - loss: 2.6962\n",
      "Epoch 1: saving model to ./training_checkpoints/ckpt_1.weights.h5\n",
      "172/172 [==============================] - 18s 51ms/step - loss: 2.6962\n",
      "Epoch 2/10\n",
      "171/172 [============================>.] - ETA: 0s - loss: 1.9692\n",
      "Epoch 2: saving model to ./training_checkpoints/ckpt_2.weights.h5\n",
      "172/172 [==============================] - 11s 51ms/step - loss: 1.9682\n",
      "Epoch 3/10\n",
      "172/172 [==============================] - ETA: 0s - loss: 1.6906\n",
      "Epoch 3: saving model to ./training_checkpoints/ckpt_3.weights.h5\n",
      "172/172 [==============================] - 11s 51ms/step - loss: 1.6906\n",
      "Epoch 4/10\n",
      "172/172 [==============================] - ETA: 0s - loss: 1.5320\n",
      "Epoch 4: saving model to ./training_checkpoints/ckpt_4.weights.h5\n",
      "172/172 [==============================] - 11s 53ms/step - loss: 1.5320\n",
      "Epoch 5/10\n",
      "172/172 [==============================] - ETA: 0s - loss: 1.4358\n",
      "Epoch 5: saving model to ./training_checkpoints/ckpt_5.weights.h5\n",
      "172/172 [==============================] - 11s 53ms/step - loss: 1.4358\n",
      "Epoch 6/10\n",
      "172/172 [==============================] - ETA: 0s - loss: 1.3692\n",
      "Epoch 6: saving model to ./training_checkpoints/ckpt_6.weights.h5\n",
      "172/172 [==============================] - 11s 57ms/step - loss: 1.3692\n",
      "Epoch 7/10\n",
      "172/172 [==============================] - ETA: 0s - loss: 1.3168\n",
      "Epoch 7: saving model to ./training_checkpoints/ckpt_7.weights.h5\n",
      "172/172 [==============================] - 12s 56ms/step - loss: 1.3168\n",
      "Epoch 8/10\n",
      "172/172 [==============================] - ETA: 0s - loss: 1.2724\n",
      "Epoch 8: saving model to ./training_checkpoints/ckpt_8.weights.h5\n",
      "172/172 [==============================] - 11s 56ms/step - loss: 1.2724\n",
      "Epoch 9/10\n",
      "172/172 [==============================] - ETA: 0s - loss: 1.2305\n",
      "Epoch 9: saving model to ./training_checkpoints/ckpt_9.weights.h5\n",
      "172/172 [==============================] - 11s 57ms/step - loss: 1.2305\n",
      "Epoch 10/10\n",
      "172/172 [==============================] - ETA: 0s - loss: 1.1898\n",
      "Epoch 10: saving model to ./training_checkpoints/ckpt_10.weights.h5\n",
      "172/172 [==============================] - 12s 60ms/step - loss: 1.1898\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sndhu_vKZd2k"
   },
   "source": [
    "## Generate Teks\n",
    "\n",
    "Cara termudah untuk menghasilkan teks dengan model ini adalah dengan menjalankannya dalam loop, dan menyimpan status internal model saat Anda menjalankannya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMJHlox_Zg4Z"
   },
   "source": [
    "Setiap kali Anda memanggil model, Anda memasukkan beberapa teks dan state internal. Model mengembalikan prediksi untuk karakter berikutnya dan state barunya. Masukkan kembali prediksi dan state ke model untuk terus menghasilkan teks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "J3XdEiGtZhYt"
   },
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "        # Create a mask to prevent \"[UNK]\" from being generated\n",
    "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            values=[-float('inf')]*len(skip_ids),\n",
    "            indices=skip_ids,\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        # Convert strings to token IDs\n",
    "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "        # Run the model\n",
    "        predicted_logits, states = self.model(inputs=input_ids,\n",
    "                                            states=states,\n",
    "                                            return_state=True)\n",
    "\n",
    "        # Only use the last prediction\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits/self.temperature\n",
    "\n",
    "        # Apply the prediction mask\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "        # Sample the output logits\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "        # Convert from token ids to characters\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "        return predicted_chars, states\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(OneStep, self).get_config()\n",
    "        config.update({\n",
    "            \"temperature\": self.temperature,\n",
    "            \"model\": self.model,  # Ensure your model class supports serialization\n",
    "            \"chars_from_ids\": self.chars_from_ids,  # Ensure serialization support\n",
    "            \"ids_from_chars\": self.ids_from_chars  # Ensure serialization support\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Recreate model instance from the saved configuration\n",
    "        model = tf.keras.Model.from_config(config['model_config'])\n",
    "        chars_from_ids = config['chars_from_ids']  # Ensure proper deserialization\n",
    "        ids_from_chars = config['ids_from_chars']  # Ensure proper deserialization\n",
    "        temperature = config['temperature']\n",
    "\n",
    "        return cls(model, chars_from_ids, ids_from_chars, temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "-sageHviZkrg"
   },
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f__CcuIqZmn4"
   },
   "source": [
    "Jalankan secara berulang untuk menghasilkan beberapa teks. Melihat teks yang dihasilkan, Anda akan melihat model mengetahui kapan harus menggunakan huruf besar, membuat paragraf, dan meniru kosakata menulis seperti Shakespeare. Karena sedikitnya jumlah epoch pelatihan, model belum belajar membentuk kalimat runtut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4T1YXGIOZorb",
    "outputId": "48c9ebc9-d1c2-4b6f-be68-debbd5e48459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "He's overgo's and in his imit them! Ithise\n",
      "Munn'd, my father still nothing by him then he was\n",
      "in their offices; and sworn at home,\n",
      "For he is made of poor Leliber.\n",
      "\n",
      "FLORIZEL:\n",
      "Our ornament,\n",
      "Or free she sweet Isabel's state, which will\n",
      "forget of perim a hard to yea,\n",
      "Set me the truth of trial Venotoes'? Take thou\n",
      "Shall, pray your love put upon our late still.\n",
      "\n",
      "GREMIO:\n",
      "Fear our purposeens are in HEDRUSTEN:\n",
      "Son that's it, stand of an aptance with the husband;\n",
      "God we much dispersent am I that hath\n",
      "been women: be with envidone here,\n",
      "Brother 'I will hear the fairest of his fires?\n",
      "O, let me three quickly, graceing wink; why, say\n",
      "'Camul to her a nothing.\n",
      "\n",
      "Clown:\n",
      "Alice, be so beteech your exbaliaber's life,\n",
      "He three much tribunes undone to end;\n",
      "Or if thou boli'ga, of brief,\n",
      "So early give our sea: and\n",
      "why still, whom I hear suppers to be\n",
      "dance, belongs in him that stuff'd him he, an amoratal\n",
      "looker and st before without our sweethery.\n",
      "\n",
      "LEONTES:\n",
      "Dinest perceive your own counsel, where he shows betw \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 3.219169855117798\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYxl7m6WqA3o"
   },
   "source": [
    "Hal termudah yang dapat Anda lakukan untuk meningkatkan hasil adalah dengan melatihnya lebih lama (coba EPOCHS = 30). Anda juga dapat bereksperimen dengan string awal yang berbeda, mencoba menambahkan lapisan RNN lain untuk meningkatkan akurasi model, atau menyesuaikan parameter suhu untuk menghasilkan prediksi yang kurang lebih acak.\n",
    "\n",
    "Jika Anda ingin model menghasilkan teks lebih cepat, hal termudah yang dapat Anda lakukan adalah membuat teks secara batch. Pada contoh di bawah, model menghasilkan 5 keluaran dalam waktu yang hampir sama dengan waktu yang dibutuhkan untuk menghasilkan 1 keluaran di atas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F0NR8fwKqBhY",
    "outputId": "337c62d5-2479-49bd-86ae-14eb46c22a7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b\"ROMEO:\\nRomeoited, do not strimpt, I wore your succers.\\n\\nISABELLA:\\nThat fair sir, for ever I but curse\\nLivio is, I warranted it be so fight,\\nAnd for I have done--more: as by gafes as when the birl.\\n\\nKATHARINA:\\nI have ta'en this tongue conceal'd the yore,\\nAnd what we will stand fellows is dead,\\nYou comesh with time of blession is.\\n\\nGREGORY:\\nHe hath not Rome, 'tis much, that I'll fell impo\\nundone, findlence of a divine, a mindeller,\\nAppoor'd my daughter.\\n\\nDUKE VINCENTIO:\\nNot swear of her, is never friend.\\nHere, stave! I have it be seat my vew to one of;\\nAnd when your brother is decheard, to our suits are\\nshe, as least a whip: go parting it.\\nGentle ungroth wence: out of this?\\n\\nFirst Keeper:\\nWas that dotrection'd wave in worthful thy head\\nMay in bleeding in the bloody blows, afterworn!\\nWhererel, the tender'st a-compassion whipp'd you scar?\\nThe dismallance's wife can bear to zenom of sweet\\nMeasure it, to help him him as we pawn,\\nAs till will be here fellowful for the Richard know\\nThe weapon of the\"\n",
      " b\"ROMEO:\\nOur scorns to begs; but asas eyes may sit by my hand.\\n\\nRICHARD:\\nNow to see him what your gods, King Edward\\nAs you henr of them, the cur doth consumed\\nThat wounds this apparen'd nature's plain.\\nThis as I wore lives it was fores and ere I spoke\\nThat he willingled from what aid you have been\\nThan this bestire with Choler, o'erchange och of this dear\\nAs prescience and Vostall's uptrem this ord\\nTo you and, of all my age; and this the manner,\\nAnd sunseing no man whise her lord.\\n\\nKING RICHARD III:\\nShall you grace must be,\\nHe shall braw another statewhries further than my rest;\\nAnd this like least to move,\\nAll zeal and strink'st that dumb'd our successions,\\nLeft thirst to cry oath, which you have recogned\\nIs pash of herm of Rome with gentlemen.\\n\\nWARWICK:\\nAnd he, that hold this vain we ender it,\\nI am this chair, if I should know thy cook'd,\\nUn hers datch'd outward: show you well,\\nOf that I hope to much a give now to be\\nSpoke to be of obdens' bursh,--yound of kings beholder,\\nAnd call'd to pleas\"\n",
      " b\"ROMEO:\\nO, more.\\n\\nHENRY BOLINGBROKE:\\nExerilies, I rejoice, I feel a franht.\\nFind fellow shall not chance hit ear, and stains\\nThe vanities and her being from offended;\\nWhich in him he is not ignorance,\\nThrough thy pleasures in our preeters are war\\nTwen your greeness and bruns; who is the phope of\\nThat that she will not know the duke and all\\nThe which my son is warm, provo to gnat again.\\n\\nDUKE OF AUMERLE:\\nYes, Henry, wronged; but no leave, take not to no troy,\\nAnd strail'd within the noble king of mine,\\n\\nKATHARINA:\\nI spake no other while. Dother 'twas a med bring\\nto her o'clock: so go done! if in my life be found\\nWith giving like him inkeed, she will practisching\\nCorioli, to be sure shortly of your\\nexecution. But if not white heaven showl'd,\\nWhen this do think it will not plead a grave\\nYour witness this rudeful lumb; the souls of Lancaster,\\nDran it and you dare hash vide a world,\\nHer is as leap'd princely for what made me truth\\nWhy drip by our intents i' the death,\\nNot a so lengthen plasts.\\n\\nLE\"\n",
      " b\"ROMEO:\\nNot bite but Richard,\\nWho ill-bellows in a noblemager;\\nWhen hast thou sir, she'll held thy nighwnant of his purpose.\\n\\nHERMIONE:\\nO, what! we stain?\\n\\nRIVERS:\\nThy noble Grounded is all gone.\\n\\nHERMIONE:\\nHe is, as the heavenly kiss'd,\\nThough man so way. Fair Bohemia of:\\nSirrah, tell him a honey voin-corror in Vienna,\\nSo. No, I'll do thee does, God grant it: it's pont\\nWhich in him be so much disdoised.\\nMushy, he wause gose after the water.\\n\\nCORIOLANUS:\\nI have amore-his remesses; let us dream\\nAnd lear them light on fine pinas one!\\nOff witerflitter slevies Romeo,--\\nFor they af officer and garden him as ever\\nNow indeed. I know not to; a ruch.\\n\\nCAMILLO:\\nO lad without your highness, here rencent-rife.\\n\\nHENRY BOLINGBROKE:\\nWhat is the child?\\n\\nCATESBY:\\nI came from him: I never call'd them.\\n\\nCORIOLANUS:\\nIf levie he that knowing we wish for his arm.\\n\\nROMEO:\\nCall him at his right:\\nThou comest fine keeps Bianca movel and e'er\\na very worn: away, yes, and whereof hims, 'after!\\n\\nCORIOLANUS:\\nYou getter is \"\n",
      " b\"ROMEO:\\nA did the mighty mourn your viedle\\nHouse on himwell homely sit at all, alival of you\\nLord Against in RoSalina.\\nO Paris, know, who is beats herself.\\n\\nLEONTES:\\nO doth.\\nPetture, peace; fie, fiery thou shall be\\nFrinking, peace!' 'ZOPRLAND:\\nRichard, hollow memorsciles to ligh\\nand consult of interveinnep strengths dissentry:\\nThen against them abroad with me;\\nAnd tell you were a custom'd puer of his\\nbarnain is the weak. Now I be remeable,\\nIffer'd, what you have seen me with Warwick\\nWith this meet opposent news.\\n\\nANGELO:\\nWell, is it now, rememberous' dyford; of the friar,\\nIf then I must wish pinal his grace.\\n\\nQUEEN ELIZABETH:\\nO, weapon wauld our least, and she can perso\\n\\nISABELLA:\\nThen, of the common, I do deserve and who here.\\n\\nCATESBY:\\nMan, and longer eyes, sisterion, yours,\\nWith giving resonce whiles in my pleen\\nThe cirit o'er new in his ovide,\\nBid him been so younger in rowerives\\nWhich elough make heard him dares his day wonters\\nIf hanged before much unto the heavens!\\nCittis granden consc\"], shape=(5,), dtype=string) \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 3.1553783416748047\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result, '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXZW62zAqF4N"
   },
   "source": [
    "## Ekspor Model Generator\n",
    "\n",
    "Model satu langkah ini dapat dengan mudah disimpan dan digunakan kembali, memungkinkan Anda menggunakannya di mana pun tf.saved_model diterima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dAeJZZCkqKfF",
    "outputId": "cea3d87d-789e-4bf6-a842-f4ecd1574893"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7beed210bd90>, because it is not built.\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = tf.saved_model.load('one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LQ9W9yvHqOLE",
    "outputId": "cd76b026-5a8d-4da4-b5a7-3d1756a2736b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "One of your best of an ear, and still\n",
      "The shepherd keeps of light on thy oate,\n",
      "Than Dukes of the ci\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tg3yxfFBbqFx"
   },
   "source": [
    "# Tugas Praktikum\n",
    "\n",
    "Prosedur pelatihan pada praktikum 2 merupakan prosedur sederhana, yang tidak memberi Anda banyak kendali. Model ini menggunakan \"teacher-forcing\" yang mencegah prediksi buruk diumpankan kembali ke model, sehingga model tidak pernah belajar untuk pulih dari kesalahan. Jadi, setelah Anda melihat cara menjalankan model secara manual, selanjutnya Anda akan mengimplementasikan custom loop pelatihan. Hal ini memberikan titik awal jika, misalnya, Anda ingin menerapkan pembelajaran kurikulum untuk membantu menstabilkan keluaran open-loop model. Bagian terpenting dari loop pelatihan khusus adalah fungsi langkah pelatihan.\n",
    "\n",
    "Gunakan tf.GradientTape untuk men track nilai gradient. Anda dapat mempelajari lebih lanjut tentang pendekatan ini dengan membaca eager execution guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpTcy3mQbz4t"
   },
   "source": [
    "1. Jalankan Model dan hitung loss dengan tf.GradientTape.\n",
    "\n",
    "2. Hitung update dan terapkan pada model dengan optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "cgi87hXvcHGQ"
   },
   "outputs": [],
   "source": [
    "class CustomTraining(MyModel):\n",
    "  @tf.function\n",
    "  def train_step(self, inputs):\n",
    "      inputs, labels = inputs\n",
    "      with tf.GradientTape() as tape:\n",
    "          predictions = self(inputs, training=True)\n",
    "          loss = self.loss(labels, predictions)\n",
    "      grads = tape.gradient(loss, model.trainable_variables)\n",
    "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "      return {'loss': loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcNkonS8ctZI"
   },
   "source": [
    "Kode diatas menerapkan train_step method sesuai dengan  Keras' train_step conventions. Ini opsional, tetapi memungkinkan Anda mengubah perilaku langkah pelatihan dan tetap menggunakan keras Model.compile and Model.fit methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "fxacJQjSb2BN"
   },
   "outputs": [],
   "source": [
    "model = CustomTraining(\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "GhDfarVScIgp"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "juQcCPz7cpQC",
    "outputId": "8d4d01cd-1708-4adc-da47-5982aae867e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 16s 61ms/step - loss: 2.7141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7beec8158550>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6Kmkn-oc0Zn"
   },
   "source": [
    "Atau jika ingin lebih mengetahui dalamnya, kita bisa membuat custom training loop sendiri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SgpMFCHkc1Mm",
    "outputId": "cc5e881e-becf-4f40-9761-c35f7db3ec1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.1593\n",
      "Epoch 1 Batch 50 Loss 2.0467\n",
      "Epoch 1 Batch 100 Loss 1.9785\n",
      "Epoch 1 Batch 150 Loss 1.8629\n",
      "\n",
      "Epoch 1 Loss: 1.9773\n",
      "Time taken for 1 epoch 13.51 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 2 Batch 0 Loss 1.7698\n",
      "Epoch 2 Batch 50 Loss 1.7618\n",
      "Epoch 2 Batch 100 Loss 1.6808\n",
      "Epoch 2 Batch 150 Loss 1.6907\n",
      "\n",
      "Epoch 2 Loss: 1.7024\n",
      "Time taken for 1 epoch 12.14 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 3 Batch 0 Loss 1.6244\n",
      "Epoch 3 Batch 50 Loss 1.5993\n",
      "Epoch 3 Batch 100 Loss 1.5423\n",
      "Epoch 3 Batch 150 Loss 1.5035\n",
      "\n",
      "Epoch 3 Loss: 1.5453\n",
      "Time taken for 1 epoch 12.13 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 4 Batch 0 Loss 1.4719\n",
      "Epoch 4 Batch 50 Loss 1.4722\n",
      "Epoch 4 Batch 100 Loss 1.4670\n",
      "Epoch 4 Batch 150 Loss 1.4313\n",
      "\n",
      "Epoch 4 Loss: 1.4477\n",
      "Time taken for 1 epoch 20.47 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 5 Batch 0 Loss 1.4235\n",
      "Epoch 5 Batch 50 Loss 1.3299\n",
      "Epoch 5 Batch 100 Loss 1.3627\n",
      "Epoch 5 Batch 150 Loss 1.3857\n",
      "\n",
      "Epoch 5 Loss: 1.3807\n",
      "Time taken for 1 epoch 11.59 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 6 Batch 0 Loss 1.3251\n",
      "Epoch 6 Batch 50 Loss 1.3399\n",
      "Epoch 6 Batch 100 Loss 1.3278\n",
      "Epoch 6 Batch 150 Loss 1.3238\n",
      "\n",
      "Epoch 6 Loss: 1.3286\n",
      "Time taken for 1 epoch 11.75 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 7 Batch 0 Loss 1.2773\n",
      "Epoch 7 Batch 50 Loss 1.2765\n",
      "Epoch 7 Batch 100 Loss 1.3021\n",
      "Epoch 7 Batch 150 Loss 1.2573\n",
      "\n",
      "Epoch 7 Loss: 1.2837\n",
      "Time taken for 1 epoch 12.13 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 8 Batch 0 Loss 1.2379\n",
      "Epoch 8 Batch 50 Loss 1.2284\n",
      "Epoch 8 Batch 100 Loss 1.1995\n",
      "Epoch 8 Batch 150 Loss 1.2419\n",
      "\n",
      "Epoch 8 Loss: 1.2429\n",
      "Time taken for 1 epoch 12.19 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 9 Batch 0 Loss 1.1857\n",
      "Epoch 9 Batch 50 Loss 1.2104\n",
      "Epoch 9 Batch 100 Loss 1.2181\n",
      "Epoch 9 Batch 150 Loss 1.2206\n",
      "\n",
      "Epoch 9 Loss: 1.2025\n",
      "Time taken for 1 epoch 12.03 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 10 Batch 0 Loss 1.1241\n",
      "Epoch 10 Batch 50 Loss 1.1363\n",
      "Epoch 10 Batch 100 Loss 1.1661\n",
      "Epoch 10 Batch 150 Loss 1.1829\n",
      "\n",
      "Epoch 10 Loss: 1.1636\n",
      "Time taken for 1 epoch 11.89 sec\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "mean = tf.metrics.Mean()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    mean.reset_states()\n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "        logs = model.train_step([inp, target])\n",
    "        mean.update_state(logs['loss'])\n",
    "\n",
    "        if batch_n % 50 == 0:\n",
    "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
    "            print(template)\n",
    "\n",
    "    # saving (checkpoint) the model every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "    print()\n",
    "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
    "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
    "    print(\"_\"*80)\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jalankan kode diatas dan sebutkan perbedaanya dengan praktikum 2?\n",
    "\n",
    "**Jawab:**\n",
    "\n",
    "Pada praktikum 2, prosedur pelatihan yang digunakan merupakan metode sederhana yang mengandalkan pendekatan teacher-forcing, di mana model tidak dilatih untuk mengatasi kesalahan prediksi sendiri. Dengan metode ini, ketika model membuat kesalahan, prediksi yang salah tidak diumpankan kembali ke model sehingga model tidak pernah belajar dari kesalahan. \n",
    "\n",
    "Pendekatan custom training loop yang ditunjukkan pada tugas praktikum ini memperkenalkan lebih banyak kontrol dan fleksibilitas selama proses pelatihan. Diamana, tf.GradientTape digunakan untuk melacak dan menghitung gradient berdasarkan loss, sehingga memungkinkan kita untuk memperbarui parameter model secara langsung menggunakan optimizer. Langkah pelatihan khusus ini diimplementasikan dalam metode train_step, yang memberikan lebih banyak kebebasan dalam mengatur perilaku pelatihan pada setiap batch data. Kita dapat mencatat loss pada batch tertentu, yang memudahkan pemantauan kinerja model secara real-time selama pelatihan, serta menyimpan model secara otomatis pada interval tertentu untuk mencegah hilangnya hasil pelatihan ketika terjadi penghentian mendadak.\n",
    "\n",
    "Selain fleksibilitas pada setiap batch, custom training loop menawarkan kontrol yang besar pada setiap epoch dengan menghitung rata-rata loss menggunakan tf.metrics.Mean(). hal ini tentunya memberikan gambaran yang lebih baik tentang kinerja model di setiap epoch. Pencatatan loss secara berkala pada setiap batch ke-50, serta kemampuan menyimpan model (checkpoint) setiap 5 epoch, memungkinkan kita untuk menerapkan strategi lanjutan seperti curriculum learning untuk menstabilkan keluaran model. Dengan metode ini, model tidak hanya berfokus pada hasil jangka pendek tetapi juga belajar memperbaiki kesalahan dalam urutan data, sehingga meningkatkan keakuratam model terhadap prediksi yang salah di masa depan.\n",
    "\n",
    "Dengan custom training loop, kita mendapatkan lebih banyak kendali atas proses pelatihan, yang berguna untuk mengimplementasikan teknik-teknik seperti curriculum learning atau pengaturan khusus lainnya yang dapat menstabilkan pelatihan model."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
